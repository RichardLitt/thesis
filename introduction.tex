\section{Introduction}\label{sec:intro}

At least half of the world's 7000 languages will be extinct this century \citep[p. 27]{grenoble_2011}. Just over half of these languages have writing systems.\footnote{\href{https://www.ethnologue.com/enterprise-faq/how-many-languages-world-are-unwritten-0}{https://www.ethnologue.com/enterprise-faq/how-many-languages-world-are-unwritten-0}} It is estimated that less than 5\% of the world's languages will be used online or have significant digital presence \citep{kornai2013digital}.

The majority of technological infrastructure used globally has been built with English, and serves English speakers. There are a few languages - perhaps thirty - with the combination of large populations with internet access, official governmental status, and Westernised, industrial economies which affords them a foothold on the web.\footnote{\href{https://w3techs.com/technologies/history\_overview/content\_language}{https://w3techs.com/technologies/history\_overview/content\_language}}
%TODO Include these languages later
In part, these languages depend upon shared code. Put simply, a literacy system affords written corpora, written corpora can be used by researchers to build grammars, and grammars can be used to build tools such as spell-checkers, parsers, input systems, or later on speech recognition and generation software, semantic analyzers, machine learning and translation systems, and so on. This culturally shared body of code is most often developed in closed environments with consumer endpoints, by the military or large businesses. For instance, the World Wide Web, the largest shared corpus of written language, started with support from  Massachusetts Institute of Technology (MIT) and  Defense Advanced Research Projects Agency (DARPA). Another example would be Google Translate, which uses massive bilingual corpora to provide automatic translation services for free online, but whose code is proprietary and owned by Google.

While this pathway works well for large languages where populations of speakers can be leveraged to provide funding, the majority of the world's languages are not able to develop their own computational resources - either grammars, corpora, or code. Instead, they must rely on small groups of researchers, limited funding, and a grab-bag of written resources when they have them (for instance, the most consistent translations cross-linguistically is the Christian bible, which may not the target language's culture).

In this thesis, I will examine methodology that can be used by linguists, researchers, and language developers to help their languages "digitally ascend" (as \citet{kornai2013digital} puts it) - to bootstrap their corpora creation, write grammars, transform other language's tools and research to their own languages, and to ultimately enable their communities to speak and share their knowledge computationally. This methodology goes under the broad label of \textit{open source} software. Open source software is code which has been developed and made available for free, without concessions about how it is to be used or who uses it. This allows coders to use code which they personally haven't built without earmarking funds for it, thus freeing up significant portions of research and development costs for making tools. At present, the majority of the world's code depends on some point on open source software - for instance, UNIX, and much of the World Wide Web, depends on open source code.

In the field of computational linguistics, however, there are a deficit of resources which are licensed and available as open source. This largely stems from the need to financially recoup expenses for development, on licenses mandated by research groups or military funders, and on a lack of awareness of how open source code works. Another consideration is that an open source label does not ensure that the code is worth using, maintained, relevant, or in scope for a given domain.

% TODO Use we instead of i
% This thesis is structured as follows: in Section~\ref{sec:drt}, we give necessary background information to \sdrt{} by introducing \drt{} and motivating the use of rhetorical relations. In Section~\ref{sec:defining}, we lay out the ground work of our implementation by properly defining \sdrs{}s and the constraints the are imposed on them by their implicit hierarchical structure. In Section~\ref{sec:comp}, we present our account of compositionality in \sdrt{}, including a novel way of modeling the augmentation of \sdrs{}s and a way to underspecify \sdrs{}s by lambda abstraction. In Section~\ref{sec:disc}, we discuss crucial choices that we made during the implementation and elaborate on some of the edge cases of \sdrt{} which we link to possible future work. , we present our conclusion.


Below, I will go into further depth about the state of linguistics and computational resources, and what different languages need in order to have digital presence. I'll define what open source is, and talk about issues relevant to open source code for under-resourced languages; specifically, data rights, liability, privacy, funding, military and industrial concerns, ethical reasons for using open source. I'll then talk about the state of open source code currently available online, in particular focusing on a database of open source code that I have built with the help of researchers around the world. Finally, I'll touch on some specific examples of languages which could benefit from open source code, focusing on Gaelic, an endangered language with tens of thousands of speakers but little online resources, and Naskapi, an endangered languages with only a thousand speakers which might be able to benefit from open source code. The Naskapi case study will be based largely on original research, as I engaged in field research at the town where most Naskapi live and talk to linguists working on literacy efforts for this language. Finally, in Section~\ref{sec:concl} I'll discuss further work possible to help endangered languages, and offer some concluding remarks.