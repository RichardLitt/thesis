% !TEX root = thesis.tex
\section{Open Source Code for Low Resource Languages}
\label{sec:endlangcode}

Now that low resource languages have been described, and now that there has been a brief overview of open source as a software methodology, the reader will doubtless wonder - what is the state of open source code that can be used today by language communities? 

% Removed as we've covered this in the Resources chapter, enough
% \subsection{BLARK and beyond}
% \label{subsec:blark-and-beyond}

Unfortunately, due to the decentralised nature of open source, this is an inherently difficult question to answer. In the ecosystem, there are a few strategies that can be used to inform an answer: look at what resources are available from any of the main large data aggregators mentioned in Section~\ref{subsec:resource-aggregators}, take a screenshot of the ecosystem based on some of the more-cited open source tool used for low resource language NLP, examine linked open data databases, and sample relevant work on GitHub through an manually collated list of resources. Each of these strategies is employed in a subsection, below.

\subsection{Low resource language NLP available through data providers}
\label{subsec:lrl-nlp-through-providers}

Starting with the lower end of the language resource pyramid, CLDR has instructions on checking out their open source subversion repository 

% http://www.elda.org/en/catalogues/language-resources-announcements/
% http://www.elsnet.org/

\subsection{Popular open source libraries}
\label{subsec:popular-open-source-libraries}

\subsubsection{The Natural Language Toolkit}
Here, I'll explain some open source resources that can be used to bootstrap development; for instance, \href{NLTK (Natural Language Toolkit)}{http://nltk.org/}, a free and open source library which uses the Python language by \citet{bird2006nltk}, and enables users to interface with over fifty different corpora and lexical resources.

A primer written by the main creators, \href{Natural Language Processing with Python}(http://nltk.org/book), is used frequently in natural language processing classes written by the creators. It is licensed under the Apache 2.0 license, a common license \footnote{https://github.com/nltk/nltk/blob/develop/LICENSE.txt}. On GitHub, there are currently 204 contributors listed \href{https://github.com/nltk/nltk/graphs/contributors}, although the git history shows 234 (found by using the command {\tt git authors}).% TODO Explain
Some of the resources within NLTK have to do with low resource languages. For instance, in 2015, NLTK added machine translation libraries, including popular ones such as IBM Models 1-3 and BLEU.

By open sourcing their code, the NLTK authors have allowed it to be adapted and re-used. Currently, there are several ports. % TODO cite.
One of these is the JavaScript language implementation, \href{https://github.com/NaturalNode/natural}{https://github.com/NaturalNode/natural}. This has 6700 stars on GitHub, which is a good indicator of community vitality and use, and 88 contributors. The port is also open source, under an MIT license \href{https://github.com/NaturalNode/natural\#license}{https://github.com/NaturalNode/natural\#license}.

\subsection{Other resources}

% % Other stuff
Not all research that is code based can be easily quantified as open source. For instance, Afranaph\footnote{\href{http://www.africananaphora.rutgers.edu/home-mainmenu-1}{http://www.africananaphora.rutgers.edu/home-mainmenu-1}} is a database of research on African languages. However, there is no code directly available to build your own database. Instead, you only have the option of searching their database. Other sites may use open source technology, but not be open source themselves. For instance, TransNewGuinea\footnote{\href{http://transnewguinea.org/about}{http://transnewguinea.org/about}} has a colophon where they mention that they use Unicode, Django, Bootstrap, jQuery, Leaflet, PostgreSQL, and SQLite.

Keyboard layouts are another area where much i18n work has been focused. Link: https://github.com/HughP/MLKA

Lack of sharing code or storing it usefully, due to factors: funding, academic cycle, inability, scope, lack of knowledge of domain

Specific examples of cross-language applicability of an open source coding library (such as NLTK, or more specifically, family-related usage of parsers or MT models), and what that says about the incentives and use cases for open source libraries.

\subsubsection{i18n documentation for larger open source tools}

In some cases, tools themselves may canonically be used for NLP, but may also be translated into LRLs, thus allowing low resource language developers to use the code themselves for bootstrapping their tools. For example, Node has an i18n and l10n committee that works to translate tools - and there is some interested in working with LRLs. % Blah blah blah maybe this is a stretch? 
Another instance would be code which has been ported into rare languages % cite uspanteko work in java

% Removed as we cover this, basically, in Open Source
%\subsection{Data permanence and interoperability}
%\label{subsec:data-permanence-and-interoperability}
%
%Here I'll talk about the inextricable nature of linguistics data and code, and how they are linked.
%
%% TODO Cite Austin Principles for Data AustinPrinciples2017

\subsection{Data and privacy}
\label{subsec:data-and-privacy}

Here, I'll talk specifically about data rights and privacy, in regards to whether it makes sense to decouple code from data, especially in cases of low resource languages, where sparse data may be naturally enriched with annotation schemas and hard to separate out from the tools being used. In such cases, how do we as a community, researchers as providers, and developers as consumers, deal with licensing, privacy, and proprietary data? Does it make sense to provide links to code that can be used institutionally or commercially without also allowing for things like royalties for usage, or proper licensing for data? Bound up in this are also ethical concerns - well studied in theoretical field linguistics - about the language users themselves not wishing for their data to be used in certain ways.

% From Chiarcos:

% a few years back, I compiled a massive corpus of Bibles and related texts
%in a CES-conformant XML format (following Resnik 1996), some also with
%annotations. For the most part, distributing this corpus would be illegal
%under European copyright law (and that's why you haven't heard about it),
%but I realized that there are circumstances which could allow
%dissemination of a great part of it under an academic license.

% Compiling and distributing a web corpus is basically illegal in Europe
%unless explicitly permitted by an accompanying license. However, US law
%has the concept of fair use, and if a data provider declares US
%legislation to apply (e.g., that "[t]hese Terms and Conditions ... are
%governed by the laws of the State of New York"), we Europeans can rely on
%the principle of fair use, as well.
%
%% According to 17 U.S.C. ยง 107, "the fair use of a copyrighted work,
%including such use by reproduction in copies or phonorecords or by any
%other means specified by that section, for purposes such as criticism,
%comment, news reporting, teaching (including multiple copies for classroom
%use), scholarship, or research, is not an infringement of copyright." The
%intended use is for NLP research, DH scholarship and classroom use, so
%that would probably not an issue -- and in fact, there is no financial
%damage whatsoever as this data is freely and redundantly available from
%the web.
%
%% However, am I allowed to distribute this corpus with an explicit license
%statement? I think CC-BY-NC should protect the intellectual and commercial
%interests of the creator of the electronic edition and be roughly in the
%spirit of an academic license, but of course, I'm not the actual owner of
%the data, but only responsible for its transformation and annotation. I am
%wondering about the consequences if someone eventually creates an NLP tool
%chain from this data and uses any models trained on the data in a
%commercial application. As the original copyright extends to derived
%works, this would be a clear violation of my license statement, of course,
%but I would be responsible as I redistributed the data and by transforming
%it from messy HTML to proper markup, I actually enabled this violation.

\subsection{Linked open data}
\label{subsec:lod}

Here, I'll briefly talk about related efforts with the Open Linguistics Working Group's \citep{chiarcos2012open} work on open source data reflected on the semantic web.\citep{chiarcos2013building}

\subsection{A database for open source code}
\label{sec:solutions}

Here, I'll talk about a database of open source code. Specifically, I'll mention my own work building \href{https://github.com/RichardLitt/endangered-languages}{https://github.com/RichardLitt/endangered-languages}, described first in \citet{CCURL}, and what it contains and who has worked on it with me. I'll cover the main tools, what kind of tools were included, and why I built the database on GitHub in this way.

I'll also include diagnostics on how it has been used and how the tools it mentions have been used - what percentage have been downloaded, and so on.


% - Its uses (specifically)
% - Current considerations in its planning
% - reception
%   - User evaluations from other open source scientists
% - Future goals

%
% - Case study using endangered-languages repository
%   - Clean up resource
%     - Add all listed resources in issues
%     - Contact and create the LSA CELP Technology Subcommittee
%     - Clone all SourceForge repositories
%     - Rename to low-resource-languages
%     - "List quality"
%     - "the pages and subpages are often dead"
%   - Get diagnostics on the state of the links I've found:
%     - What percentage have been updated when
%     - Downloaded, etc.
%   - Review Excel results


% TODO Mention caballero.pdf
% This was a linguistic fieldwork exercise; they made their own tools for resource management in ELAN https://github.com/ucsd-field-lab/kwaras In the paper, they use Word, Quicktime, ELAN, GitHub... but there's not much open source code, at all. Most of it is shimmied together.
