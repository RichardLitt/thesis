\section{Introduction}\label{sec:intro}

At least half of the world's 7000 languages will be extinct this century \citep[p. 27]{grenoble_2011}. Just over half of these languages have writing systems.\footnote{\href{https://www.ethnologue.com/enterprise-faq/how-many-languages-world-are-unwritten-0}{https://www.ethnologue.com/enterprise-faq/how-many-languages-world-are-unwritten-0}} It is estimated that less than 5\% of the world's languages will be used online or have significant digital presence \citep{kornai2013digital}.

The majority of technological infrastructure used globally has been built with English, and serves English speakers. There are a few languages - perhaps thirty - with the combination of large populations with internet access, official governmental status, and Westernised, industrial economies which affords them a foothold on the web.\footnote{\href{https://w3techs.com/technologies/history\_overview/content\_language}{https://w3techs.com/technologies/history\_overview/content\_language}}
%TODO Include these languages later
In part, these languages depend upon shared code. Put simply, a literacy system affords written corpora, written corpora can be used by researchers to build grammars, and grammars can be used to build tools such as spell-checkers, parsers, input systems, or later on speech recognition and generation software, semantic analysers, machine learning and translation systems, and so on.
% Alexis: the grammar step feels slightly muddied -- I'm not entirely convinced that grammars are the necessary pivot... but this is a fine point we can discuss later
This culturally shared body of code is most often developed in closed environments with consumer endpoints, by the military or large businesses. For instance, the World Wide Web, the largest shared corpus of written language, started with support from  the Massachusetts Institute of Technology (MIT) and the Defense Advanced Research Projects Agency (DARPA). Another example would be Google Translate, which uses massive bilingual corpora to provide automatic translation services for free online, but whose code is proprietary and owned by Google.

While this pathway works well for large languages where populations of speakers can be leveraged to provide funding, the majority of the world's languages are not able to develop their own computational resources - either grammars, corpora, or code. Instead, they must rely on small groups of researchers, limited funding, and a grab-bag of written resources when they have them. For instance, the most consistent translations cross-linguistically are of the Christian bible, which may not reflect the target language's culture.
% Alexis: also what about spoken language corpora? e.g. work by Oliver Adams & colleagues inducing linguistic information from spoken data, skipping the usual transcription set. I'm not saying you need to also deal with spoken language data, but it does need to be acknowledged

In this thesis, I will examine methodology that can be used by linguists, researchers, and language developers to help their languages "digitally ascend" (as \citet{kornai2013digital} puts it) - to bootstrap their corpora creation, write grammars, transform other language's tools and research to their own languages, and to ultimately enable their communities to speak and share their knowledge computationally. This methodology goes under the broad label of \textit{open source} software. Open source software is code which has been developed and made available for free, without concessions about how it is to be used or who uses it. This allows coders to use code which they personally haven't built without allocating funds for it, thus freeing up significant portions of research and development costs for making tools. At present, the majority of the world's code depends on some level on open source software - for instance, Linux, and much of the World Wide Web, depends on open source code.

In the field of computational linguistics, however, there are a deficit of resources which are licensed and available as open source. This largely stems from the need to financially recoup expenses for development, on licenses mandated by research groups or military funders, and on a lack of awareness of how open source code works by developers. Another consideration is that an open source label does not ensure that the code is worth using, maintained, relevant, or in scope for a given domain.

Below, I will go into further depth about the state of endangered languages and computational resources in Section~\ref{sec:endlang}, and what different languages need in order to have digital presence. In Section~\ref{sec:open-source}, I'll define what open source is, and talk about issues relevant to open source code for under-resourced languages; specifically, data rights, liability, privacy, funding, military and industrial concerns, ethical reasons for using open source. I'll then in Section~\ref{sec:endlangcode} talk about the state of open source code currently available online, in particular focusing on a database of open source code that I have built with the help of researchers around the world. I'll touch on some specific examples of languages which could benefit from open source code in Section~\ref{sec:case-studies}, focusing on Gaelic, an endangered language with tens of thousands of speakers but little online resources, and Naskapi, an endangered languages with only a thousand speakers which might be able to benefit from open source code. The Naskapi case study will be based largely on original research, as I engaged in field research at the town where most Naskapi live and talk to linguists working on literacy efforts for this language. In Section~\ref{sec:discussion} I'll discuss how open source can help low resource languages. Finally, in Section~\ref{sec:future-work} and Section~\ref{sec:conclusion} I'll discuss future work, and offer some concluding remarks.
