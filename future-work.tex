% !TEX root = thesis.tex
\section{Future Work}
\label{sec:future-work}

This thesis, a cursory look at open source and low resource languages, has highlighted more than a few directions for future research. I cover some of these below.

\subsection{Extending databases of OSS code for LRLs}

The LRE Map resource is fantastic in that it has an order of magnitude more resources listed than other aggregators, like OLAC. With 2000 resources, it should be a port of call for linguists and researchers working on LRLs. Similarly, the database provided by Kornai's lab to measure language vitality has great potential, as it examines many aspects of digital language presence that are not mentioned in the other large typological or reference databases. However, both fall short in a very important way; they do not link to the resources they mention. Ideally, they should be portals for language developers hoping to work on their specific languages. Extending these portals - or, as a last resort, making a new one - would be beneficial to language communities hoping to increase the scope of their language's digital presence. This is an area of ripe future research; the low-resource-languages list presented in Section~\ref{sec:solutions} is only a hint at might be possible with properly aggregated metadata.

\subsection{Rethinking metrics for digital presence}
\citet{kornai2013digital} was a seminal, groundbreaking analysis of language presence on the web. \citet{gibson2016assessing} was a good extension. \citet{soria2017digital} extends these even further, to better approximate digital language vitality. However, it excluded heritage languages, and does not judge specific languages to test its applicability. As this was a draft to elicit feedback (Soria, personal communication), they can be forgiven for this; but this is then, clearly, an area of future research. As I pointed out, there is no scale as of yet that ranks English on its own ranking, either, where it ought to be given its global digital dominance. And, finally, there is not a good metric that combines quantitative and qualitative measurements together, although \citepos{kornai2015new} inclusion of SIL approaches this. While this would be difficult, I think that it could be possible.

Another interesting avenue of research would be to analyse metrics for digital presence for constructed languages. While scoping a metric is understandable, excluding entire communities because they are centred around \textit{a priori} languages seems, to me, to be rash, especially as these languages often have a strong identify function for their speakers.

\subsection{Rethinking language diversity and typological relation}

\citet{ginsburgh2011many} asked how many languages we really need; \citet{bender2010grand, bender2016linguistic} touched on the field of language typology being a resource for multilingual projection of language models and tooling. I was unable to find research on the number of languages which could functionality have multilingual NLP applied to them in the world. It ought to be possible to calculate a number of languages for which automatic hunspell dictionaries could be elicited; and then automatic POS taggers; and then automatic grammar models, and so on. Put succinctly, if Irish morphological parsers can be applied to Gaelic, could they also be applied to Cornish? And if they can be applied to Cornish, then it might be possible to cluster all of the Goidelic languages in one grouping, such that the amount of languages which require language development could be thought of less as individual entities but as groupings of typological features which can multilingual NLP can be adapted for. This would be an interesting area of research, although it is unclear how easy it would be to grasp the low-hanging fruit.

\subsection{Metrics for code usage in LREC or ACL papers}

It should be possible to mine ACL or LREC papers for references to open source storage repositories. While this was not done for this paper, but a quantitative study of academic research and open source code storage would greatly facilitate discussion around linguistic coding and tooling. This could also go hand in hand with extending the Austin Principles of Data Citation in Linguistics \citet{AustinPrinciples2017} to apply to code objects, and by providing DOIs to all of the repositories extracted.

\subsection{Development of an p2p storage system for linguistics code}

While data collectors like META-SHARE go a long way towards collecting metadata, it would be fantastic to develop a p2p collection system for open source code in linguistics, using the process briefly outlined in Section~\ref{subsec:sharing-code-without-a-platform}. I have already put a subset of repositories listed on low-resource-languages into IPFS, by using a shell script to automatically extract all GitHub repositories from the list, to clone (download) them from GitHub, and then to add them into IPFS. Ideally, these repositories could then be pinned in IPFS by other nodes, which would involve publishing the process and results in an academic paper and then collaborating with research bodies and individual developers to help replicate the data. Ideally, a permanent, decentralised network could be created for scientific software, beyond linguistics data. This is an exciting area.

One suggestion would be to also invent a cryptocurrency to incentivise storage of linguistic data on a blockchain: "putting linguistics on the blockchain", as a colleague laughingly joked when I explained it. While initially humorous due to the dubious longevity of blockchain projects, the idea is technologically interesting, and warrants further research.

\subsection{Extending Gaelic and Naskapi resources}

The research here has highlighted many areas of research which could be opened on Gaelic and Naskapi. For instance, an \textit{n}-gram MT system for Gaelic may be feasible given the amount of bilingual data. For Naskapi, there is work to be done implementing UIs for syllabics that could be used in Facebook, Snapchat, Instagram, and other venues where there are lots of speakers using language technology. If nothing else, it would be interesting to run a quantitative study examining how much Naskapi is currently being used on social media by the Naskapi community, or to ask the Band Office for as many bilingual texts as they have to develop a semi-supervised MT system. More work here is needed.

%
%\subsection{Beyond Ethnologue}
%
%Ethnologue holds the keys of the ISO 639-3 standard, widely used and accepted around the globe. However, it is also a proprietary service, and has a paywall which limits usage. Glottolog and other open source efforts go a long way towards providing alternatives - Glottolog keeps its own ISO standards, for instance.
%
%\subsection{Beyond Wikipedia}
%
%I will talk about the shortcomings of both Wikipedia as a service, and Ethnologue as a provider of language data. Specifically, I want to draw attention to how Wikipedia treats its long-term contributors, and how Ethnologue charges exorbitant fees for using its data, and what we can do to improve this.

% \subsection{An Open Data Repository}

% I will spec out the plans for an open data repository that could be used to share data.

% - Peer-to-peer solution for sharing code
%   - Stub out example
%   - Build a web searcher for automatically getting and sharing code
%     Further Work:
%   - Open source data repositories (touch on)
%   - Working with Ethnologue


% \subsection{Storage on a p2p network}
%
% Build a web-application tool for serving a decentralized data store for endangered language tools and data
%
% Example:
%
% I have already put a subset of repositories listed on endangered-languages into IPFS, a p2p resource for storing and disseminating data in a decentralized and persistent fashion.
%
% Process:
%
% 1. `cat` the endangered-languages README.md, then `grep` for `/.*(//github\.com/.*?/[a-zA-Z0-9-]*).*/` (all github.com repos).
% 2. Output list into separate file.
% 3. `awk` the first few repos, until a random divider, and clone the git repos: `awk '1;/kuromoji-server/{exit}' ../githublist.md | xargs -n1 git clone`
% 4. `ipfs add -r repos`
% 5. `ipfs pin add repos`
