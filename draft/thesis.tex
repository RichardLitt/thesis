\documentclass[10pt, a4paper]{article}
\usepackage{../LREC2016/lrec2016}
\usepackage{multibib, url}
\newcites{languageresource}{Language Resources}
\usepackage{graphicx}
% for eps graphics

\usepackage{epstopdf}
\usepackage[latin1]{inputenc}

\title{The State of Open Source Resources for Low Resource Languages}

\name{Richard Littauer}

\address{Saarland University\\
         Saarbr\"ucken, Germany\\
         richard.littauer@gmail.com\\}


\abstract{
% TODO
% We present a database of open source code that can be used by low-resource language
% communities and developers to build digital resources. Our database is also useful to software developers
% working with those communities and to researchers looking to describe the state of the field
% when seeking funding for development projects.  \\
% \newline
% \Keywords{open source, under-resourced languages, database, endangered languages, code, computational resources}
}

\begin{document}

\maketitleabstract

\section{Introduction}

At least half of the world's 7000 languages will be extinct this century. Grenoble 2011:27 (Bourget 2015)

Of the world's roughly 7000 languages, only a small number are present on the World Wide Web, or present in digital form. The majority of technological infrastructure that first world nations use and depend upon has been built with English, and serves English speakers. There are a few languages with large populations and state-backing which have a large foothold on the web. For instance, China is estimated to have more users of the internet than Japan, India, and the United States combined \footnote{\url{http://chinapower.csis.org/web-connectedness/}{http://chinapower.csis.org/web-connectedness/}}. The majority of these users will be speakers of Standard Mandarin, and not English; the operating systems and infrastructural backbone will still depend upon English-language originating software, but the user interface will be in Chinese. Some thirty languages % TODO \cite{Crubadan}
 are estimated to be at this level. The majority of the world's languages, however, have no significant user presence, and are not present on the World Wide Web.

There are various metrics would can be used to assess linguistic health in the digital sphere.

% Would be good to be able to cite something here. Otherwise it is original research.
\begin{itemize}
\item It is spoken by living fluent speakers, including second-language learners.
\item It is spoken by living, first-language learners.
\item It is productive in it's morphology, growing in vocabulary, and not frozen in time.
\item It is recorded in some form, including audio files.
\item It has a writing system.
\item It has a writing system that is used by modern speakers to record their own language.
\item It has a writing system that can be used on a computer.
\item The electronic writing system does not require excessive installation.
\item All normal characters are available in Unicode.
\item There is a growing corpus of written documents in the language.
\item There are users who consistently use the language digitally.
\item There is a formalized spelling system.
\item There is a Bible translation. %Write about Wycliffe and SIL
\item There are non-electronic documents.
\item There is a dictionary.
\item There is a machine-readable corpus.
\item It is used on modern social media; Twitter, and so on.
\item There is a Wikipedia entry.
\item There are spellcheckers.
\item There are syntactic tools.
\item There are machine learning algorithms based on the language.
\item There are speech-to-text or text-to-speech systems developed for the language.
\end{itemize}


- Definition of computational linguistics, and linguistic tooling
- Code as it pertains to lrl
- state of the field linguistically
- State of the field computationally
- Lack of sharing code or storing it usefully, due to factors: funding, academic cycle, inability, scope, lack of knowledge of domain
- Some shared code

In this paper, I will talk about:

- Open source code
  -  Longevity of linguistic scholarship and work
  - Data, rights, liability, and privacy
  - Funding
    - Institutional bottleneck
    - Linguistic colonialism
    - Ethical and moral concerns for military usage
    - Ethical and moral concerns for big business usage
- Open Source work currently available
  - Case study on GitHub, SourceForge, some archival sites (UPenn, Max Planck, DFKI)
- Case study using endangered-languages repository
  - Clean up resource
  - Get diagnostics on the state of the links I've found:
    - What percentage have been updated when
    - Downloaded, etc.
  - Review Excel results
- Peer-to-peer solution for sharing code
  - Stub out example
  - Build a web searcher for automatically getting and sharing code
Further Work:
  - Open source data repositories (touch on)
  - Working with Ethnologue
Conculusion



\section{Open Source code}

What it is, the history of it in Computational Linguistics and elsewhere, and various incentive models for using open source methods

% Other stuff
Not all research that is code based can be easily quantified as open source. For instance, [Afranaph](http://www.africananaphora.rutgers.edu/home-mainmenu-1) is a database of research on African languages. However, there is no code directly available to build your own database. Instead, you only have the option of searching their database. Other sites may use open source technology, but not be open source themselves. For instance, [TransNewGuinea](http://transnewguinea.org/about) has a colophon where they mention that they use Unicode, Django, Bootstrap, jQuery, Leaflet, PostgreSQL, and SQLite.

Keyboard layouts are another area where much i18n work has been focused. Link: https://github.com/HughP/MLKA

\section{Data and privacy}

Whether it makes sense to decouple code from data, especially in cases of low resource languages, where sparse data may be naturally enriched with annotation schemas and hard to separate out from the tools being used. In such cases, how do we as a community, researchers as providers, and developers as consumers, deal with licensing, privacy, and proprietary data? Does it make sense to provide links to code that can be used institutionally or commercially without also allowing for things like royalties for usage, or proper licensing for data? Bound up in this are also ethical concerns - well studied in theoretical field linguistics - about the language users themselves not wishing for their data to be used in certain ways.

% From Chiarcos:

% a few years back, I compiled a massive corpus of Bibles and related texts
%in a CES-conformant XML format (following Resnik 1996), some also with
%annotations. For the most part, distributing this corpus would be illegal
%under European copyright law (and that's why you haven't heard about it),
%but I realized that there are circumstances which could allow
%dissemination of a great part of it under an academic license.

% Compiling and distributing a web corpus is basically illegal in Europe
%unless explicitly permitted by an accompanying license. However, US law
%has the concept of fair use, and if a data provider declares US
%legislation to apply (e.g., that "[t]hese Terms and Conditions ... are
%governed by the laws of the State of New York"), we Europeans can rely on
%the principle of fair use, as well.
%
%% According to 17 U.S.C. ยง 107, "the fair use of a copyrighted work,
%including such use by reproduction in copies or phonorecords or by any
%other means specified by that section, for purposes such as criticism,
%comment, news reporting, teaching (including multiple copies for classroom
%use), scholarship, or research, is not an infringement of copyright." The
%intended use is for NLP research, DH scholarship and classroom use, so
%that would probably not an issue -- and in fact, there is no financial
%damage whatsoever as this data is freely and redundantly available from
%the web.
%
%% However, am I allowed to distribute this corpus with an explicit license
%statement? I think CC-BY-NC should protect the intellectual and commercial
%interests of the creator of the electronic edition and be roughly in the
%spirit of an academic license, but of course, I'm not the actual owner of
%the data, but only responsible for its transformation and annotation. I am
%wondering about the consequences if someone eventually creates an NLP tool
%chain from this data and uses any models trained on the data in a
%commercial application. As the original copyright extends to derived
%works, this would be a clear violation of my license statement, of course,
%but I would be responsible as I redistributed the data and by transforming
%it from messy HTML to proper markup, I actually enabled this violation.


\section{Funding}

IARPA and DARPA both are involved with low resource languages and both of them may have their own institutional values that are probably at ends with independent researchers, commercial consumers, and language communities. Does working on sparse data openly bring along with it ethical or moral concerns; if so, how can these be adequately explained, breached, and talked about? How can they be worked around or be part of the conversation? Note that DARPA and the like also use humanitarian reasons as their primary stated aim for work on sparse languages, which may be contrary to their military needs. There is already an extensive literature on moral uses of data -- I could summarize that, and apply it specifically to low resource languages, which is something I do not think has yet been published.

\section{Digital Permanence and Storage}

Universities and institutions have short timelines and are largely dependent on specific, allocated, and thus finite funding. What other models are there for data storage? What concerns are there?

\section{Choosing Repositories}

Longer term plans for open source repositories; GitHub is useful currently, but it also a business, and as such its aims may not be aligned with its users. I would like to talk about building a database of open source repositories on a secure, permanent, peer-to-peer network. This is something I am actively involved in professionally (I currently work at IPFS, which is building such a network). I would like to talk about linguistic and scientific applications of using versioned, p2p, and distributed systems for storing both open source code related to low resource languages as well as language data.

\section{Language Specific Needs}

- Disambiguate low-resource language, minority languages, endangered languages, and sparse languages (among others) are used often synonymously, but are distinct and come along with different stakeholders and communities, which means different values, methods, and goals.
- A review of low resource language resources and their target communities and languages, in general; a state of the field for the issue.
- Specific examples of cross-language applicability of an open source coding library (such as NLTK, or more specifically, family-related usage of parsers or MT models), and what that says about the incentives and use cases for open source libraries.

\subsection{Some Thoughts on NLTK}

\href{NLTK (Natural Language Toolkit)}{http://nltk.org/} is an free and open source library which uses the Python language, and enables users to interface with over fifty different corpora and lexical resources. A primer written by the main creators, \href{Natural Language Processing with Python}(http://nltk.org/book), is used frequently in natural language processing classes written by the creators. It is licensed under the Apache 2.0 license, a common license \footnote{https://github.com/nltk/nltk/blob/develop/LICENSE.txt}. On GitHub, there are currently 204 contributors listed \href{https://github.com/nltk/nltk/graphs/contributors}, although the git history shows 234 (found by using the command `git authors` % TODO Explain
). Some of the resources within NLTK have to do with low resource languages. For instance, in 2015, NLTK added machine translation libraries, including popular ones such as IBM Models 1-3 and BLEU.

By open sourcing their code, the NLTK authors have allowed it to be adapted and re-used. Currently, there are several ports.
% TODO cite.
One of these is the JavaScript language implementation, \href{https://github.com/NaturalNode/natural}{https://github.com/NaturalNode/natural}. This has 6700 stars on GitHub, which is a good indicator of community vitality and use, and 88 contributors. The port is also open source, under an MIT license \href{https://github.com/NaturalNode/natural#license}{https://github.com/NaturalNode/natural#license}.

\section{Example Use Case}

I propose a study of RichardLitt/endangered-languages:
- It's uses (specifically)
- Current considerations in it's planning
- reception
  - User evaluations from other open source scientists
- Future goals

\section{Tool}

Build a web-application tool for serving a decentralized data store for endangered language tools and data

Example:

I have already put a subset of repositories listed on endangered-languages into IPFS, a p2p resource for storing and disseminating data in a decentralized and persistent fashion.

Process:

1. `cat` the endangered-languages README.md, then `grep` for `/.*(//github\.com/.*?/[a-zA-Z0-9-]*).*/` (all github.com repos).
2. Output list into separate file.
3. `awk` the first few repos, until a random divider, and clone the git repos: `awk '1;/kuromoji-server/{exit}' ../githublist.md | xargs -n1 git clone`
4. `ipfs add -r repos`
5. `ipfs pin add repos`

%ferences}
%\label{main:ref}
%
%\bibliographystyle{lrec2016}
%\bibliography{references.bib}


%\section{Language Resource References}
%\label{lr:ref}
%\bibliographystylelanguageresource{lrec2016}
%\bibliographylanguageresource{references.bib}

\end{document}
