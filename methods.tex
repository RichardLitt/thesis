% !TEX root = thesis.tex
\section{Methods}
\label{sec:methods}

It is customary when doing a quantitative review to give advice around best practices, to make not just the next researcher's job easier, but also to help life the quality of the state of the field, in general. In my research, I have often done the same: \citet{DBLP:journals/corr/KatzCWHVHSJCCVL15}, which came out of a workshop on sustainable software in the sciences, does a reasonable job of doing this for software citation; \citet{LittIDCC} for scientific workflows; \citet{LittEdulearn} for crowdsourcing learning materials by students in the classroom; and \citet{wiggins2013data} for public participation in science. Here, in the same vein, are some recommendations for utilising open source for LRL NLP.

\subsection{Choosing a license}
\label{choosing-a-license}

Legal advice on the internet is often preceded by the initialism IANAL, stating "I am not a lawyer", or sometimes "I am not your lawyer." The following is not meant to constitute legal advice, and I am not liable for any advice given here.

That having been said, licensing software in the open domain is definitely to been encouraged. Section~\ref{subsec:licenses} lists many licenses which are considered open source; any of them should work for most purposes (although I would recommend against the Unlicense, as it does not waive liability.) %TODO Cite?

\citet{streiter2006implementing} recommends using the GPL license for any software contributed into a software pool, their terminology for community-curated open source software. They also recommend the lesser GPL, as needed; however, GPL is preferred because it enforces that all modifications to software be brought back to the original moderator for acknowledgement, which allows for the source code to be updated. A specific example they give is of Scannell's Irish spell checker.

\begin{quote}
The case of Irish language spell checking is illustrative in this regard. Kevin Scannell developed an Irish spell checker and morphology engine in 2000, integrated it into the Ispell pool, and released everything under the GPL. Independent work at Microsoft Ireland and Trinity College Dublin led to a Microsoft-licensed Irish spell checker in 2002, but with no source code or word lists made freely available. Now, roughly five years later, the GPL tool has been updated a dozen times thanks to contributions from the community, and the data have been used directly in several advanced NLP tools, including a grammar checker and an MT system. The closed-source word list has not, to our knowledge, been updated at all since its initial release. Indeed, a version of the free word list, repackaged for use with Microsoft Word, has all but supplanted use of the Microsoft-licensed tool in the Irish-speaking community. \citep[282-283]{streiter2006implementing}
\end{quote}

I would recommend against GPL for another reason; code is often maintained by a single author, and GPL puts undo pressure on the author to maintain the code in the long term. Maintenance of code is difficult, as it involves work time that is often not paid, and as it requires that the author of the code sets expectations around levels of maintenance.

For this reason, I have always licensed my own code under the MIT license, which waives all liability and insists that the code therein is provided as-is. This makes long term maintenance easier on the maintainers, as it removes undue pressure to keep code updated. On the other hand, this leads to abandonware - code which is released into the commons and then not updated, such as TileMill which \citet{gawne2016mapmaking} used in their paper, which is no longer updated. I think that this is a reasonable price to pay for stopping burnout for the maintainers, a major factor influencing coders leaving open source. % TOOD Cite?

It is worth noting that work published without a license on a public site is not technically open source. When software is not licensed, it by default reverts (in the US legal jurisdiction, anyway) to copyright where {\it all rights are reserved}, which is by definition not FLOSS. For this reason, it is important to add a license to code if it is in your purview to do so, and if you wish to follow the open source methodology.

\subsection{Choosing repositories}
\label{choosing-repositories}

Choosing repositories is another question which needs to be answered if code is to be open sourced. All of the options mentioned so far - hosting it yourself, hosting it on an academic website, using a third-party hosting company - have their costs and benefits. If you have the resources to host the code yourself, I would suggest doing so. Unfortunately, this means that your site becomes the bottleneck for entry and discovery. Academic sites, on the other hand, may be more easily accessed by researchers in the field. However, public sites - like GitHub - are where most open source code lives, as was established in Section~\ref{subsec:where-is-open-source-code}.

For this reason, I explicitly recommend using GitHub as a storage space for open source code. Unfortunately, GitHub is a private company, and its long term goals may not align with scientists interested in century-long timelines. The Rosetta Project,\footnote{\href{https://rosettaproject.org/}{https://rosettaproject.org/}. \last{April~27}} run by the Long Now Foundation, aims to store human languages for millennia - and forward thinking on this length, while not normally used by academic researchers, raises the question of how long code ought to be stored and whether or not short term solutions are adequate.

I mentioned briefly in Section~\ref{sec:solutions} that I mirrored all of the Sourceforge repositories I found onto GitHub. Mirroring involves copying an entire code base - importantly, along with the license, so that there is no mistaking authorship - to another ecosystem or service, to maintain it in the long run. It is for this purpose that I set up the GitHub organisation @LowResourceLanguages\footnote{\href{https://github.com/lowresourcelanguages}{https://github.com/lowresourcelanguages}. \last{April~27}} (tangentially connected with the similarly named low-resource-languages repository). This organisation works as a shell to mirror code archives which might otherwise be lost.

I highly recommend mirroring all of the code that you open source, not only on GitHub, but on your personal server if you have one, and, if possible, within @LowResourceLanguages. This affords maximal accessibility, longevity, and indexing within the vibrant GitHub ecosystem.

\subsection{Sharing code without a platform}
\label{subsec:sharing-code-without-a-platform}

Of course, each of these three servers depends upon single points of failure: either your server, your provider, or your academic host. Ideally, the code would exist within large organisations to serve, as well, but there currently is no centralised codebase for linguistic code resources. OLAC, META-SHARE, LRE Maps, LingHub, LinguistList, and the LLOD all are aggregators, not hosts of code. As far as I am aware, @LowResourceLanguages on GitHub is the only code base which explicitly hosts the code. But it also relies upon GitHub's presence; which may change in ten, twenty, or a hundred years.

Peer-to-peer (p2p) technology may provide a solution to this. These work by using protocols to communicate between nodes in a network. Each node holds a copy of the file and any node which wants a copy can get it from any other node which has it. The more nodes hold a file, the easier and faster this transfer process becomes; and, if one node goes down, the other nodes can still transmit files. This allows for data permanence on a level which is unknown on on the HTTP and TCP based web.

IPFS, the InterPlanetary File System,\footnote{\href{https://ipfs.io/}{https://ipfs.io/}. \last{April~27}} is one such system which could be used to host data in the long term. The Dat project is another similar project,\footnote{\href{https://datproject.org/}{https://datproject.org/}. \last{April~27}} which has been used to save data which was deleted during by the Trump administration from US governmental websites.\footnote{\href{https://medium.com/@maxogden/project-svalbard-a-metadata-vault-for-research-data-7088239177ab}{https://medium.com/@maxogden/project-svalbard-a-metadata-vault-for-research-data-7088239177ab}. \last{April~27}} Both of these systems use hashes - deterministic DOIs based on data, which are part of the system that underly the Git tool used by GitHub and other researchers - to point to content, as opposed to locations. This allows for faster connections, offline usage with connected nodes that are not connected to the web itself, less link rot, greater specificity of content, and decentralisation.

Without going into too much detail, storing data on IPFS and then sharing it between nodes is trivial. For instance, the JSON data\footnote{\href{https://gist.github.com/RichardLitt/e60bcf9f399939b16181bf25ad6da8ba}{Available at https://gist.github.com/RichardLitt/e60bcf9f399939b16181bf25ad6da8ba}. \last{April~26}} used to analyse the low-resource-languages repository in Section~\ref{sec:solutions} could be uploaded to IPFS by installing the program and then running: {\tt ipfs add data.json}. This returns a hash (DOI) which points to the data: {\tt QmPztYpkC3aSs\-MYKDcod\-3wJtvoivbp\-NDfxNKQ6dwxnzA52}. This hash can be shared by anyone who runs IPFS, meaning that they are now storing the code on their own device, as well. It can also be accessed through a gateway to IPFS: for instance, by going to \href{http://ipfs.io/ipfs/QmPztYpkC3aSsMYKDcod3wJtvoivbpNDfxNKQ6dwxnzA52}{\nolinkurl{http://ipfs.io/ipfs/QmPztYpkC3aSsMYKDcod3wJtvoivbpNDfxNKQ6dwxnzA52}}. Uptime may depend upon the \href{https://ipfs.io}{https://ipfs.io} gateway. The code will always be available within the IPFS network for anyone who accesses it at that hash, regardless of whether the gateway is up or not. This is similar to RDF and a SPARQL gateway, except that the underpinning logic does not depend upon XML specifications, but the data itself.

There are more applications than just storing data, however. Some similar projects are already being used by non-central language communities. For instance, Guyanese communities are using p2p systems combined with GIS to map illegal logging on their land, all while being offline and not being connected to the main internet.\footnote{\href{https://www.digital-democracy.org/}{https://www.digital-democracy.org/}. \last{April~27}} \citet[90]{jancewicz2002applied} talked at length about how Naskapi development benefited from a linguist working hand-in-hand with local communities, versus long-distance arrangements as with Cree, which resulted in slower uptake of tooling and in adverse standardisation of syllabics and keymapping. A p2p network could help in these environments. It could also be used to share linguistic data within a language community, without depending upon an institutional archive in another country, a significant barrier to access and licensing control for language communities.
